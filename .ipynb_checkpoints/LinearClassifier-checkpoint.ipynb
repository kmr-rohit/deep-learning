{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5a367e-cec0-4757-bd5f-98b623e00be2",
   "metadata": {},
   "source": [
    "We can divide a set of points on a givin line using linear algebra as : \n",
    "$f(x, w) = w_1*x_1+w_2$\n",
    "\n",
    "\n",
    "We can then extend the same idea to 2D Plane : \n",
    "$f(x,w) = w_1*x_1 +w_2*x_2+ w_3$\n",
    "\n",
    "This idea can be naturally extended to arbitrary (N) dimensions. As a result, we now have $f(x,\\mathbf{W}) =\\mathbf{W}*x +b$ , where $x$ is N-dimensional vector , $b$ is a scaler , while $W$ is a $1 X N$ matrix\n",
    "\n",
    "In Pytorch, we can build a linear classifier with 5 inputs and 10 outputs using just one line of code. The following code will initialize a trainable matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77384fbf-2868-423c-ac75-edd593ee4ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.4.7/libexec/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: networkx, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed networkx-3.5 torch-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38890869-3728-4f77-8b84-503014ff3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b91854-ed68-4e28-9043-ed0b57789563",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(5, 10)\n",
    "# print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4e233-b15b-40b7-b554-7478e5971a76",
   "metadata": {},
   "source": [
    "Training the classifier. We know that we need to find $W$ and $b$ in order to classify our points. But how do we do that ? First o fall , we need training data , i.e we need data points - $x$ and target $t$ , which we are aware of. \n",
    "\n",
    "Before going to the process of training, we must know 2 new concepts. \n",
    "#### 1. Loss Function :\n",
    "   It is the measure of how good or bad classification of the data point is. More precisely.\n",
    "   Given a dataset ${(x_i,t_i)}$ of N points , where $x_i$ is a N-dimensional point in space and $t_i$ is an integer that defines the points category , loss is the distance between $f(x_i, W)$ and $C_i( f(x_i,W),t_i)$ is the cost for a single example $x_i$. The overall loss of the entire training data is simply the average of all the individual losses. However, in practice, we rarely average the loss over all data points.\n",
    "\n",
    "   There are multiple choices of Loss Functions , we will have a look at :\n",
    "   $C= \\sum(f(x_i,W) - t_i)^2$\n",
    "\n",
    "Lets see how we can write the same in `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128c4abd-cf6f-4c8c-a4f1-3c380963d805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  tensor([ 0.4646, -1.0671, -0.2893], grad_fn=<ViewBackward0>)\n",
      "Output (Loss):  tensor(1.0056, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple linear model: input dimension=10, output dimension=3\n",
    "# This means the model will map a 10-dimensional vector to 3 output values.\n",
    "model = nn.Linear(10, 3)\n",
    "\n",
    "# Define a Mean Squared Error (MSE) loss function.\n",
    "# It measures the squared difference between predicted and target values.\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Create a dummy input vector of size 10 (random values sampled from a normal distribution).\n",
    "# This simulates one data point with 10 features.\n",
    "input_vector = torch.randn(10)\n",
    "\n",
    "# Define the target output vector.\n",
    "# Here we assume a 3-class problem, and the target is class index 2 (represented as one-hot [0,0,1]).\n",
    "target = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "# Forward pass: pass the input through the model to get predictions (size 3).\n",
    "pred = model(input_vector)\n",
    "\n",
    "# Compute the loss between predicted vector and target vector.\n",
    "output = loss(pred, target)\n",
    "\n",
    "# Print results\n",
    "print(\"Prediction: \", pred)   # The raw output from the linear model (logits, not probabilities).\n",
    "print(\"Output (Loss): \", output)  # The scalar MSE loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465cebd-6946-4d2e-9292-cc343f408f8b",
   "metadata": {},
   "source": [
    "#### 2. Optimization and training process \n",
    "Optimization is the process of finding the weight matrix $W$ that minimizes the loss function. In other words, it is the process of selecting the individual weights $w_i$ so that the classifier’s prediction $y$ is as close as possible to the point’s real label $t$\n",
    "\n",
    "\n",
    "Now, we can describe the training algorithm in its entirety:\n",
    "\n",
    "Given a set of training examples $x_i$ with their labels $t_i$ , we need to:\n",
    "\n",
    " - Initialize the classifier $f(x_i,\\mathbf{W})$ with random weight $W$ . \n",
    " - Feed a training example in the classifier and get the output $y_i$ .\n",
    " - Compute the loss between $y_i$ the prediction $t_i$ .\n",
    " - Adjust the weights $W$ according to the loss $C_i$ .\n",
    " - Repeat for all training examples.\n",
    "   \n",
    "This is the core idea behind all deep learning models. In the end, we will have a trained classifier that can be generalized in previous unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4fa64-eccf-40a4-b1a3-8bc7ac4b1d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
